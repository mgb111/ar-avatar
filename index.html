<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
<title>AI Avatar Lip-Sync & AR - Enhanced</title>
<script src="https://cdn.tailwindcss.com"></script>
<style>
    body { font-family: 'Inter', sans-serif; margin:0; overflow:hidden; background-color:#111827; color:#f3f4f6; }
    canvas { display:block; width:100%; height:100%; }
    #ar-button-container { position:absolute; top:0; left:0; right:0; bottom:0; display:flex; justify-content:center; align-items:center; z-index:20; }
    #ar-button { background-color: rgba(31,41,55,0.8); color:white; padding:1rem 2rem; font-size:1.25rem; border-radius:9999px; border:1px solid rgba(75,85,99,0.8); cursor:pointer; font-weight:600; box-shadow:0 4px 6px rgba(0,0,0,0.1); transition: all 0.2s; }
    #ar-button:hover { background-color: rgba(55,65,81,0.9); }
    #loading-overlay, #ar-instructions { position:absolute; top:0; left:0; width:100%; height:100%; display:flex; flex-direction:column; justify-content:center; align-items:center; z-index:100; color:#d1d5db; font-size:1.125rem; background-color: rgba(17,24,39,0.9); text-align:center; padding:1rem; }
    #ar-instructions { display:none; }
    #status-message { position:absolute; top:2rem; left:50%; transform:translateX(-50%); background-color: rgba(239,68,68,0.8); color:white; padding:0.5rem 1rem; border-radius:0.5rem; z-index:110; font-size:0.875rem; display:none; text-align:center; }
    /* debug panel removed */
    .loader, .thinking-loader { border:4px solid #4b5563; border-top:4px solid #60a5fa; border-radius:50%; width:40px; height:40px; animation:spin 1s linear infinite; }
    .loader { margin-bottom:1rem; }
    @keyframes spin { 0%{transform:rotate(0deg);} 100%{transform:rotate(360deg);} }
    #mic-level { width:60px; height:10px; background:#374151; border-radius:5px; overflow:hidden; margin-top:4px; }
    #mic-level-fill { width:0%; height:100%; background:#10b981; transition:width 0.05s; }
</style>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>

<div id="loading-overlay">
    <div class="loader"></div>
    <span>Loading 3D environment...</span>
</div>

<div id="ar-instructions">
    <span>Move your phone to find a flat surface, then tap to place the avatar.</span>
</div>

<div id="status-message"></div>

<div id="ar-button-container">
    <button id="ar-button-placeholder">Enter Augmented Reality</button>
</div>

<div id="ar-controls-container" style="position:absolute; bottom:2rem; left:50%; transform:translateX(-50%); z-index:10; display:none; flex-direction:column; align-items:center; gap:0.5rem;">
    <button id="mic-button" class="w-16 h-16 bg-blue-600 hover:bg-blue-700 text-white font-bold p-4 rounded-full transition duration-200 shadow-lg flex justify-center items-center">
        <svg id="mic-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2"></path><line x1="12" y1="19" x2="12" y2="23"></line></svg>
        <div id="thinking-icon" class="thinking-loader" style="display:none; width:24px; height:24px;"></div>
    </button>
    <div id="mic-level"><div id="mic-level-fill"></div></div>
    <!-- Text input fallback -->
    <div style="display:flex; gap:0.5rem; width:90vw; max-width:520px;">
        <input id="text-input" type="text" placeholder="Type your prompt..." class="flex-1 px-3 py-2 rounded-md bg-gray-800 text-gray-100 border border-gray-600 focus:outline-none focus:ring-2 focus:ring-blue-500" />
        <button id="send-button" class="px-4 py-2 rounded-md bg-green-600 hover:bg-green-700 text-white font-semibold">Send</button>
    </div>
</div>

<script async src="https://unpkg.com/es-module-shims@1.8.0/dist/es-module-shims.js"></script>
<script type="importmap">
{
    "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
        "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/"
    }
}
</script>

<script type="module">
import * as THREE from 'three';
import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
import { ARButton } from 'three/addons/webxr/ARButton.js';

// Backend proxy base URL (host your Node backend on Vercel/Render/Railway/Fly)
// Example: https://your-backend.example.com
const BACKEND_URL = 'https://ar-avatar.vercel.app';
// Event context used to guide the avatar's responses
const EVENT_CONTEXT = `Company Overview:
majorbeam is a creative technology agency based in Vashi, Navi Mumbai, specializing in immersive Augmented Reality (AR) and Virtual Reality (VR) solutions. The company focuses on combining technology with design to create tools that improve visual communication, streamline workflows, and drive sales conversions.

Leadership Team:
- Manish Bhanushali (Co-founder & Tech Lead): Brings a decade of experience in XR solutions, 360b0 media, and technical sales.
- Amrita Ramesh (Co-founder & Creative Lead): Specializes in visual communication, psychology, and design principles.

Core Solutions & Services:
majorbeam offers a suite of specialized XR products tailored to specific industry needs. Below is a breakdown of their offerings, including benefits, target audiences, and approximate pricing.

1. majorbeam 360b0 RealView
A high-fidelity 360b0 virtual walkthrough platform featuring interactive storytelling, voiceovers, and info hotspots.
Key Benefits: Reduces unnecessary physical visits by ~40%, extends reach to remote/international buyers, and accelerates decision-making (users spend 5x longer on these listings).
Ideal For: Real estate developers, resorts/hospitality, retail spaces, and CDMOs/labs.
Pricing: Starts at 92.5 Lakhs (depending on area and complexity).

2. majorbeam XR SpaceFormer
An extended reality platform for spatial visualization and real-time configuration of interior spaces.
Key Benefits: Enables instant visualization of customizable interiors, reducing design revisions and rework, and acts as a virtual showroom for unlimited SKUs without physical constraints.
Ideal For: Interior designers, product vendors, and real estate developers.
Pricing: Starts at 92.5 Lakhs.

3. majorbeam AR LayOut
An AR tool that allows users to arrange and compose 3D elements in real-world spaces to visualize the final result.
Key Benefits: Provides instant size clarity, validates dimensions on-site, optimizes pathways, and prevents costly layout errors.
Ideal For: Architects, equipment suppliers, and turnkey solution providers.
Pricing: Starts at 92.5 Lakhs.

4. majorbeam AR MagicLabel
Enriches static marketing materials (brochures, packaging) with 2D/3D visuals that come to life when scanned.
Key Benefits: Boosts engagement and recall compared to traditional print, allows content updates without reprinting, and provides trackable data on user attention.
Ideal For: FMCG/packaged goods, e-commerce, and educational materials.
Pricing: Starts at 91.2 Lakhs.

5. majorbeam AR Catalogue
An AR product catalogue enabling the placement of 3D models on real-life surfaces with detailed functional demonstrations.
Key Benefits: Replaces physical samples to cut logistics costs, shortens sales cycles, and empowers sales teams with a portable full range of working models.
Ideal For: Industrial equipment, machinery, and furniture vendors.
Pricing: Starts at 92.5 Lakhs.

6. majorbeam AR TryOn
True-to-scale virtual try-on solutions for wearables.
Key Benefits: Increases conversion rates, lowers return rates by ensuring fit accuracy, and boosts average order value.
Ideal For: Jewelry, eyewear, makeup, footwear, and apparel.
Pricing: Starts at 92.5 Lakhs.

7. majorbeam QuickTrainXR
Interactive VR training modules for workforce development.
Key Benefits: Cuts learning time (up to 4x faster), reduces training costs at scale (up to 52% savings), and enables safe practice of hazardous tasks in a virtual environment.
Ideal For: Manufacturing, pharma, logistics, energy, and heavy industries.
Pricing: Starts at 93.5 Lakhs.

8. majorbeam CinemaVR
High-fidelity 360b0 video narratives for immersive storytelling.
Key Benefits: Drives deeper emotional connection and brand recall, and can be deployed across web, social, and VR platforms.
Ideal For: D2C brands, investor pitches, CSR storytelling, and brand anniversaries.
Pricing: Starts at 92.5 Lakhs.

Portfolio & Client Base:
majorbeam has delivered XR experiences across sectors including government, real estate, pharma, and consumer brands.
Featured Projects:
- Govt of Telangana: VR showcase positioning investment potential.
- Auro Realty: Virtual tour for Kohinoor Apartments (3 & 4 BHK).
- CDMO Lab: 360b0 virtual tour of a state-of-the-art laboratory.
- Immersive 360b0 Experience: Cinematic sector storytelling.

They have collaborated with brands such as HUL (Hindustan Unilever), Mankind, JK Tyre, MPEDA, Pernod Ricard, Coromandel, Aurigene, and GIM.

Contact Information:
Website: majorbeam.com
Emails: manishb@majorbeam.com, amritar@majorbeam.com
Phones: +91 79774 10892, +91 78992 89069.`;

// --- Lip-Sync Overrides ---
// If your model uses different names, set these to force the selection.
// Example: const LIP_MESH_NAME='Head'; const LIP_MORPH_NAME='jawOpen';
// Or if using bones: const LIP_BONE_NAME='jaw';
const LIP_MESH_NAME = '';
const LIP_MORPH_NAME = '';
const LIP_BONE_NAME = '';

// --- Lip-Sync Tuning (feel free to tweak quickly) ---
// EXTREME settings to force visible lip movement
const MOUTH_BASE_GAIN = 3.0; // reasonable gain from audio amplitude
const MOUTH_SMOOTH = 0.3;   // some smoothing for natural motion
const MOUTH_SPIKE_CHANCE = 0.0; // no random spikes
const LIP_BONE_MAX_OPEN = -0.6; // conservative jaw rotation if using bones
const JAW_MORPH_GAIN = 1.3; // slightly stronger morph influence
const OPEN_VISEME_BIAS = 0.0; // no bias towards open mouth
// Do NOT force movement when not speaking
const FORCE_MOVEMENT_ALWAYS = false;
// Drive visemes from audio bands (set to true if you want extra lip shapes)
const USE_VISEMES = false;

let camera, scene, renderer, controls, model, mixer, clock, jawOpenMorphTargetIndex = null, jawMesh = null, jawBone = null, jawBoneBaseX = 0, lastJaw=0, engineModel=null;
// Viseme morph targets discovered in the model: { mesh, name, index }
let visemeTargets = [];
// Mouth pulsing state for more expressive movement
let mouthPhase=0, mouthSpeed=0.8, mouthStrength=0.8, nextMouthChange=0;
// Smooth random open/close while speaking
let mouthCurrent=0, mouthTarget=0;
let reticle, speechRecognition, isListening=false, isSpeaking=false, isAIThinking=false, currentAudio=null, audioContext, analyser, dataArray;
let hitTestSource=null, hitTestSourceRequested=false, groundPlane=null;
let isPlaced=false, allowReposition=false, longPressTimer=null, longPressStart={x:0,y:0};
const LONG_PRESS_MS=600, MOVE_TOLERANCE=10; // px
let audioSource, activeGesture=null, initialScale=new THREE.Vector3(), initialPinchDistance=0;
let isRotatingOneFinger=false, rotateStartX=0, rotateStartY=0, initialYaw=0;
let initialTwoFingerAngle=0;

const loadingOverlay=document.getElementById('loading-overlay');
const arButtonContainer=document.getElementById('ar-button-container');
const arInstructions=document.getElementById('ar-instructions');
const statusMessage=document.getElementById('status-message');
const arControlsContainer=document.getElementById('ar-controls-container');
const micButton=document.getElementById('mic-button');
const micIcon=document.getElementById('mic-icon');
const thinkingIcon=document.getElementById('thinking-icon');
const micLevelFill=document.getElementById('mic-level-fill');
const textInput=document.getElementById('text-input');
const sendButton=document.getElementById('send-button');

// debug elements removed

const defaultAvatarUrl='https://raw.githubusercontent.com/mgb111/ar-avatar/main/avatar.glb';
const engineModelUrl='./3d_printable_radial_pneumatic_engine.glb';
const ENGINE_OFFSET = new THREE.Vector3(0.6, 0, 0); // offset to the right of the avatar
const aiQueue=[];

init();

function init(){
    // Backend URL check (silent)
    if(!BACKEND_URL){ /* no-op */ }

    scene=new THREE.Scene();
    clock=new THREE.Clock();
    camera=new THREE.PerspectiveCamera(75,window.innerWidth/window.innerHeight,0.1,1000);
    camera.position.set(0,1.6,2.5);

    // Audio context
    try{ audioContext=new (window.AudioContext||window.webkitAudioContext)(); } catch(e){ /* no-op */ }

    // WebXR Support
    if('xr' in navigator){ navigator.xr.isSessionSupported('immersive-ar').then(()=>{}); }

    const ambientLight=new THREE.AmbientLight(0xffffff,1.5); scene.add(ambientLight);
    const directionalLight=new THREE.DirectionalLight(0xffffff,2.5); directionalLight.position.set(1,1,1); scene.add(directionalLight);

    renderer=new THREE.WebGLRenderer({ antialias:true, alpha:true });
    renderer.setSize(window.innerWidth,window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio);
    renderer.xr.enabled=true;
    document.body.appendChild(renderer.domElement);

    controls=new OrbitControls(camera,renderer.domElement);
    controls.target.set(0,1,0);
    controls.update();

    loadModel(defaultAvatarUrl);
    loadEngineModel(engineModelUrl);
    setupSpeechRecognition();
    setupAR();

    micButton.addEventListener('click',toggleListening);
    if(sendButton){
        const sendText=()=>{ const v=textInput?.value?.trim(); if(!v||isSpeaking||isAIThinking) return; queueAI(v); textInput.value=''; };
        sendButton.addEventListener('click',sendText);
        textInput?.addEventListener('keydown',(e)=>{ if(e.key==='Enter'){ e.preventDefault(); sendText(); }});
    }

    window.addEventListener('resize',onWindowResize);
    renderer.domElement.addEventListener('touchstart',onTouchStart,{passive:false});
    renderer.domElement.addEventListener('touchmove',onTouchMove,{passive:false});
    renderer.domElement.addEventListener('touchend',onTouchEnd);

    renderer.setAnimationLoop(animate);
    // Start jaw animation loop even without audio so mouth moves anyhow if forced
    if(FORCE_MOVEMENT_ALWAYS){
        try{ animateJawSync(); }catch{}
    }
}

// --- Functions --- //

function showStatusMessage(message,isError=true){
    statusMessage.textContent=message;
    statusMessage.style.backgroundColor=isError?'rgba(239,68,68,0.8)':'rgba(34,197,94,0.8)';
    statusMessage.style.display='block';
    setTimeout(()=>{statusMessage.style.display='none';},5000);
}

// --- Viseme Driving --- //
function driveVisemesFromAudio(){
    if(!visemeTargets.length || !dataArray) return;
    // Compute rough band energies
    const n = dataArray.length;
    if(!n) return;
    const lowEnd = Math.max(8, Math.floor(n*0.12));
    const midEnd = Math.max(lowEnd+8, Math.floor(n*0.35));
    let low=0, mid=0, high=0;
    for(let i=0;i<lowEnd;i++) low+=dataArray[i];
    for(let i=lowEnd;i<midEnd;i++) mid+=dataArray[i];
    for(let i=midEnd;i<n;i++) high+=dataArray[i];
    low/= (lowEnd*255); mid/= ((midEnd-lowEnd)*255); high/= ((n-midEnd)*255);

    // Emphasize dynamics
    low = Math.min(1, Math.sqrt(low)*1.2 + (OPEN_VISEME_BIAS||0)); // bias towards open mouth
    mid = Math.min(1, Math.sqrt(mid)*1.2);
    high= Math.min(1, Math.sqrt(high)*1.2);

    // Build desired viseme weights (0..1)
    const weights = {};
    // Open mouth shapes from low band
    weights['aa'] = low;
    weights['ah'] = low*0.9;
    weights['open'] = low;
    // Round shapes from low-mid
    const round = Math.max(low*0.7, mid*0.5);
    weights['oh'] = round;
    weights['oo'] = round;
    weights['w'] = round*0.8;
    weights['o'] = round;
    // Spread shapes from mid-high
    const spread = Math.max(mid, high*0.6);
    weights['ee'] = spread;
    weights['ih'] = spread*0.8;
    // Teeth/lip press from high band
    weights['fv'] = high;
    weights['mbp'] = high*0.7;

    applyVisemeWeights(weights);
}

function applyVisemeWeights(weights){
    // Prepare map by mesh to avoid repeated lookups
    const byMesh = new Map();
    for(const t of visemeTargets){
        if(!byMesh.has(t.mesh)) byMesh.set(t.mesh, []);
        byMesh.get(t.mesh).push(t);
    }
    // For each mesh, zero all matched viseme morphs first for clean blending
    for(const [mesh, targets] of byMesh){
        if(!mesh.morphTargetInfluences) continue;
        for(const t of targets){ mesh.morphTargetInfluences[t.index] = 0; }
    }
    // Apply based on best name match per target
    for(const [mesh, targets] of byMesh){
        if(!mesh.morphTargetInfluences) continue;
        for(const t of targets){
            const k = t.name.toLowerCase();
            let w = 0;
            if(k.includes('aa') || k.includes('ah') || k.includes('open')) w = Math.max(weights['aa']||0, weights['ah']||0, weights['open']||0);
            else if(k.includes('oh') || k.includes('oo') || k==='o' || k.includes('round')) w = Math.max(weights['oh']||0, weights['oo']||0, weights['o']||0);
            else if(k.includes('ee') || k.includes('ih') || k.includes('spread')) w = Math.max(weights['ee']||0, weights['ih']||0);
            else if(k.includes('fv') || k.includes('teeth') || k.includes('lip') || k.includes('mbp')) w = Math.max(weights['fv']||0, weights['mbp']||0);
            else w = (weights['open']||0)*0.2;
            mesh.morphTargetInfluences[t.index] = Math.min(1, w);
        }
    }
}

// Helper to zero all discovered viseme morph targets
function zeroVisemes(){
    if(!visemeTargets.length) return;
    for(const t of visemeTargets){
        if(t.mesh && t.mesh.morphTargetInfluences && typeof t.index === 'number'){
            t.mesh.morphTargetInfluences[t.index] = 0;
        }
    }
}

// --- AR Button --- //
function setupAR(){
    const arButton=ARButton.createButton(renderer,{ requiredFeatures:['hit-test','dom-overlay'], domOverlay:{root:document.body} });
    arButton.id='ar-button';
    arButton.textContent="Enter Augmented Reality";
    arButton.addEventListener('click',()=>{ if(audioContext.state==='suspended') audioContext.resume(); });
    renderer.xr.addEventListener('sessionstart',()=>{
        arButtonContainer.style.display='none';
        arInstructions.style.display='flex';
        controls.enabled=false;
        arControlsContainer.style.display='flex';
    });
    renderer.xr.addEventListener('sessionend',()=>{
        arButtonContainer.style.display='flex';
        arControlsContainer.style.display='none';
        arInstructions.style.display='none';
        if(currentAudio) currentAudio.pause();
        controls.enabled=true;
        if(model){ model.position.set(0,0,0); model.visible=true; }
    });
    arButtonContainer.innerHTML=''; arButtonContainer.appendChild(arButton);

    reticle=new THREE.Mesh(
        new THREE.RingGeometry(0.05,0.07,32).rotateX(-Math.PI/2),
        new THREE.MeshBasicMaterial()
    );
    reticle.matrixAutoUpdate=false;
    reticle.visible=false;
    scene.add(reticle);

    let controller=renderer.xr.getController(0);
    controller.addEventListener('select',onSelect);
    scene.add(controller);
}

// --- Speech Recognition --- //
function setupSpeechRecognition(){
    const SpeechRecognition=window.SpeechRecognition||window.webkitSpeechRecognition;
    if(SpeechRecognition){
        speechRecognition=new SpeechRecognition();
        // Android Chrome works better with interimResults and sometimes continuous
        speechRecognition.continuous=true;
        speechRecognition.lang=navigator.language||'en-US';
        speechRecognition.interimResults=true;
        speechRecognition.maxAlternatives=1;

        speechRecognition.onresult=(event)=>{
            // Build the full transcript including interim results
            let interim=''; let final='';
            for(let i=event.resultIndex;i<event.results.length;i++){
                const res=event.results[i];
                if(res.isFinal) queueAI(res[0].transcript.trim());
            }
        };
        speechRecognition.onstart=()=>{ isListening=true; micButton.classList.add('bg-red-600','hover:bg-red-700'); micButton.classList.remove('bg-blue-600','hover:bg-blue-700'); };
        speechRecognition.onend=()=>{ isListening=false; micButton.classList.remove('bg-red-600','hover:bg-red-700'); micButton.classList.add('bg-blue-600','hover:bg-blue-700'); };
        speechRecognition.onaudiostart=()=>{};
        speechRecognition.onaudioend=()=>{ /* often fires quickly on Android */ };
        speechRecognition.onspeechstart=()=>{};
        speechRecognition.onspeechend=()=>{ /* recognition will finalize shortly */ };
        // Single auto-retry on no match (some Android builds require a quick restart)
        let didRetryNoMatch=false;
        speechRecognition.onnomatch=()=>{
            if(!didRetryNoMatch){
                didRetryNoMatch=true;
                try{ speechRecognition.abort(); }catch{}
                setTimeout(()=>{ try{ speechRecognition.start(); }catch{} }, 200);
            } else {
                showStatusMessage('Did not catch that. Please try again.', false);
            }
        };
        speechRecognition.onerror=(event)=>{
            let hint='';
            if(event.error==='not-allowed' || event.error==='denied') hint='Please allow microphone access in your browser settings.';
            if(event.error==='no-speech') hint='No speech detected. Try speaking closer to the mic.';
            if(event.error==='aborted') hint='Recognition aborted. Tap the mic again.';
            showStatusMessage(`Microphone error: ${event.error}. ${hint}`);
        };
        // Reset retry flag when session ends
        speechRecognition.onend=(()=>{
            const origEnd = speechRecognition.onend;
            return ()=>{
                didRetryNoMatch=false;
                isListening=false; micButton.classList.remove('bg-red-600','hover:bg-red-700'); micButton.classList.add('bg-blue-600','hover:bg-blue-700');
                if(typeof origEnd==='function') origEnd();
            };
        })();
    } else { 
        micButton.disabled=true; showStatusMessage('Voice input not supported',false); 
    }
}

function toggleListening(){
    if(isListening||isSpeaking||isAIThinking) return;
    if(currentAudio) currentAudio.pause();
    // SpeechRecognition requires secure context (https or localhost)
    const isSecure = location.protocol==='https:' || location.hostname==='localhost';
    if(!isSecure){ showStatusMessage('Voice input requires HTTPS. Please use a secure URL.'); return; }
    if(navigator.mediaDevices?.getUserMedia){
        navigator.mediaDevices.getUserMedia({audio:true}).then(()=>{
            try{ speechRecognition.abort(); }catch{}
            try{ speechRecognition.start(); }catch{ showStatusMessage('Unable to start speech recognition'); }
        }).catch(()=>{ showStatusMessage('Microphone access denied'); });
    } else if(speechRecognition){ speechRecognition.start(); } else { showStatusMessage('Voice input unavailable'); }
}

// --- AI Queue --- //
function queueAI(prompt){ aiQueue.push(prompt); if(!isAIThinking && !isSpeaking) processAIQueue(); }
async function processAIQueue(){
    if(!aiQueue.length) return;
    const prompt=aiQueue.shift();
    await getAIResponse(prompt);
    processAIQueue();
}

// --- OpenAI API --- //
async function getAIResponse(userPrompt){
    isAIThinking=true; micIcon.style.display='none'; thinkingIcon.style.display='block'; micButton.disabled=true; if(sendButton) sendButton.disabled=true; if(textInput) textInput.disabled=true;

    const systemPrompt = `You are the brand ambassador avatar for majorbeam, a creative technology agency based in Vashi, Navi Mumbai, specializing in immersive AR/VR/XR solutions for business. Be warm, concise, and conversational. Proactively describe majorbeam's capabilities, products, and relevant case studies when appropriate, and answer questions accurately using the provided context. Keep replies to 1–2 sentences unless the user asks for more details. If a question is outside the provided context, answer based on general business and XR knowledge while staying aligned with majorbeam's positioning. Context:\n\n${EVENT_CONTEXT}`;

    try{
        if(!BACKEND_URL){ throw new Error('Backend URL not configured'); }
        const response=await fetch(`${BACKEND_URL}/api/chat`,{
            method:'POST',
            headers:{'Content-Type':'application/json'},
            body:JSON.stringify({ systemPrompt, userPrompt })
        });
        if(!response.ok){
            let detail = '';
            const clone = response.clone();
            try { detail = JSON.stringify(await response.json()); }
            catch { detail = (await clone.text()).slice(0,200); }
            throw new Error(`API error ${response.status}: ${detail}`);
        }
        const data=await response.json();
        const text=data.text||"Sorry, I couldn't generate a response.";
        await generateAndPlaySpeech(text.trim());
    } catch(error){ showStatusMessage(error.message); console.error(error); }
    finally{ isAIThinking=false; micIcon.style.display='block'; thinkingIcon.style.display='none'; micButton.disabled=false; if(sendButton) sendButton.disabled=false; if(textInput) textInput.disabled=false; }
}

// --- TTS + Lip-Sync --- //
async function generateAndPlaySpeech(text){
    if(!text||!model) return;
    isSpeaking=true;

    try{
        if(!BACKEND_URL){ throw new Error('Backend URL not configured'); }
        const response=await fetch(`${BACKEND_URL}/api/tts`,{
            method:'POST',
            headers:{'Content-Type':'application/json'},
            body:JSON.stringify({ text })
        });
        if(!response.ok){
            let detail = '';
            const clone = response.clone();
            try { detail = JSON.stringify(await response.json()); }
            catch { detail = (await clone.text()).slice(0,200); }
            throw new Error(`TTS generation failed (${response.status}): ${detail}`);
        }
        // Clean up any previous playback and nodes
        try{ if(currentAudio){ currentAudio.pause(); currentAudio.src=''; } }catch{}
        try{ if(audioSource){ audioSource.disconnect(); audioSource=null; } }catch{}
        try{ if(analyser){ analyser.disconnect(); analyser=null; } }catch{}

        const blob=await response.blob();
        const audioUrl=URL.createObjectURL(blob);
        const audio=new Audio();
        audio.src = audioUrl;
        audio.preload = 'auto';
        audio.playsInline = true;
        audio.loop=false;
        currentAudio=audio;

        // Build audio graph
        if(audioContext.state==='suspended') { try{ await audioContext.resume(); }catch{} }
        audioSource=audioContext.createMediaElementSource(audio);
        analyser=audioContext.createAnalyser();
        analyser.fftSize=128; // lighter processing to reduce glitches
        dataArray=new Uint8Array(analyser.frequencyBinCount);
        audioSource.connect(analyser).connect(audioContext.destination);

        const startPlayback = async ()=>{
            try{
                await audio.play();
                animateJawSync();
            }catch(e){ console.warn('Playback start failed, trying resume', e); try{ await audioContext.resume(); await audio.play(); }catch(err){ console.error(err); showStatusMessage('Audio playback blocked'); } }
        };
        if(audio.readyState>=3){
            await startPlayback();
        } else {
            audio.addEventListener('canplaythrough', startPlayback, { once:true });
        }

        audio.onended=()=>{
            isSpeaking=false;
            jawOpenMorphTargetIndex!==null && setJaw(0);
            try{ zeroVisemes(); }catch{}
            try{ if(audioSource){ audioSource.disconnect(); audioSource=null; } }catch{}
            try{ if(analyser){ analyser.disconnect(); analyser=null; } }catch{}
            try{ URL.revokeObjectURL(audioUrl); }catch{}
        };
    } catch(e){ console.error(e); showStatusMessage('Audio playback error'); isSpeaking=false; }
}

// --- Jaw Lip-Sync --- //
function animateJawSync(){
    if(!model) return;
    requestAnimationFrame(animateJawSync);
    // When not speaking, keep mouth closed
    if(!isSpeaking){
        setJaw(0);
        zeroVisemes();
        return;
    }

    // Smooth random open/close while speaking
    const now = performance.now();
    if(now >= nextMouthChange){
        // Pick a new random target openness and next change time
        // Distribution: 45% closed (0.1-0.3), 10% medium (0.35-0.7), 45% wide (0.7-1.0)
        const r = Math.random();
        if(r < 0.45) mouthTarget = 0.1 + Math.random()*0.2;         // mostly closed
        else if(r < 0.55) mouthTarget = 0.35 + Math.random()*0.35;  // medium range
        else mouthTarget = 0.7 + Math.random()*0.3;                  // wide open
        nextMouthChange = now + 220 + Math.random()*580;             // 220..800 ms
    }
    // Interpolate smoothly towards target
    const lerp = 0.15; // smoothing factor per frame
    mouthCurrent += (mouthTarget - mouthCurrent) * lerp;
    // Optionally drive visemes if enabled
    if(USE_VISEMES && analyser && dataArray) driveVisemesFromAudio();
    // Apply the jaw value last so visemes don't override it
    setJaw(mouthCurrent);
}

function setJaw(value){
    // Smooth value to avoid jitter
    let v = Math.min(Math.max(value,0),1);
    // Non-linear boost to emphasize small amplitudes
    v = Math.sqrt(v);
    // Even faster response, minimal smoothing for snappier motion
    v = lastJaw*(MOUTH_SMOOTH) + v*(1-MOUTH_SMOOTH);
    lastJaw = v;
    if(jawMesh && jawOpenMorphTargetIndex!==null && jawMesh.morphTargetInfluences){
        // Exaggerate morph influence to open wider (allow slight overshoot beyond 1)
        jawMesh.morphTargetInfluences[jawOpenMorphTargetIndex]=Math.min(1.6, v * JAW_MORPH_GAIN);
        return;
    }
    // Fallback: rotate jaw bone around X if available
    if(jawBone){
        // Typical jaw open is rotation downwards (negative X in many rigs). Adjust factor as needed.
        const maxOpen = LIP_BONE_MAX_OPEN; // more noticeable motion
        jawBone.rotation.x = jawBoneBaseX + v * maxOpen;
        if(Math.abs(jawBone.rotation.x-jawBoneBaseX)<0.01){
            jawBone.rotation.y = jawBoneBaseX + v * maxOpen;
            if(Math.abs(jawBone.rotation.y-jawBoneBaseX)<0.01){
                jawBone.rotation.z = jawBoneBaseX + v * maxOpen;
            }
        }
    }
}

// --- Model Loading --- //
function loadModel(url){
    const loader=new GLTFLoader();
    loader.load(url, gltf=>{
        model=gltf.scene; scene.add(model); model.position.set(0,0,0); model.visible=true;
        // Ensure teeth meshes are visible
        model.traverse(child=>{
            const n=(child.name||'').toLowerCase();
            if(child.isMesh && (n.includes('teeth')||n.includes('tooth'))){ child.visible=true; }
        });
        // Find a suitable morph target for jaw/mouth opening on any mesh
        const candidates=['jawOpen','JawOpen','JAW_Open','MouthOpen','mouthOpen','openMouth','OpenMouth','viseme_aa','AA','Aa','a','A','viseme_A'];
        jawOpenMorphTargetIndex = null; jawMesh = null; jawBone = null; jawBoneBaseX = 0;
        let bestScore = -Infinity, bestMesh = null, bestIndex = null, bestMorphName = '';

        // Manual overrides: try bone override first
        if(LIP_BONE_NAME){
            model.traverse(child=>{
                if(jawBone) return;
                if(child.isBone && (child.name||'').toLowerCase().includes(LIP_BONE_NAME.toLowerCase())){
                    jawBone = child; jawBoneBaseX = child.rotation.x;
                }
            });
        }

        // Manual overrides: try morph override on a specific mesh (or any mesh)
        if(!jawBone && (LIP_MORPH_NAME || LIP_MESH_NAME)){
            model.traverse(child=>{
                if(jawMesh) return;
                if(!child.isMesh || !child.morphTargetDictionary) return;
                const meshOk = !LIP_MESH_NAME || (child.name||'').toLowerCase().includes(LIP_MESH_NAME.toLowerCase());
                if(!meshOk) return;
                const dict = child.morphTargetDictionary;
                for(const [key, idx] of Object.entries(dict)){
                    if((key||'').toLowerCase()===LIP_MORPH_NAME.toLowerCase() || (!LIP_MORPH_NAME && candidates.map(n=>n.toLowerCase()).includes((key||'').toLowerCase()))){
                        jawMesh = child; jawOpenMorphTargetIndex = idx; return;
                    }
                }
            });
        }

        // Auto-detection if overrides didn’t resolve it
        model.traverse(child=>{
            if(!child.isMesh || !child.morphTargetDictionary) return;
            const dict = child.morphTargetDictionary;
            const meshName = (child.name||'').toLowerCase();
            for(const [key, idx] of Object.entries(dict)){
                const k = key.toLowerCase();
                // Base score from morph name
                let score = 0;
                if(candidates.map(n=>n.toLowerCase()).includes(k)) score += 5;
                if(k.includes('jaw')) score += 4;
                if(k.includes('mouth')) score += 3;
                if(k.includes('open')) score += 2;
                if(k.includes('aa') || k==='a') score += 1;
                // Mesh preference
                if(meshName.includes('head')||meshName.includes('face')||meshName.includes('jaw')||meshName.includes('mouth')) score += 3;
                if(meshName.includes('eye')||meshName.includes('eyelid')||meshName.includes('brow')||meshName.includes('teeth')||meshName.includes('tongue')) score -= 5;
                if(score>bestScore){ bestScore=score; bestMesh=child; bestIndex=idx; bestMorphName=key; }

                // Collect possible viseme/mouth morphs for shaping
                if(/viseme|mouth|lip|aa|ah|oh|oo|ee|ih|uh|fv|mbp|w\b|\bo\b/i.test(k)){
                    visemeTargets.push({ mesh: child, name: key, index: idx });
                }
            }
        });
        if(!jawMesh && bestMesh && bestScore>0){ jawMesh=bestMesh; jawOpenMorphTargetIndex=bestIndex; }
        // If no morph target found, look for a jaw/mouth bone
        if(jawOpenMorphTargetIndex===null && !jawBone){
            model.traverse(child=>{
                if(jawBone) return;
                if(child.isBone){
                    const n = (child.name||'').toLowerCase();
                    if((LIP_BONE_NAME && n.includes(LIP_BONE_NAME.toLowerCase())) || n.includes('jaw') || n.includes('chin') || (n.includes('mouth') && !n.includes('upper'))){
                        jawBone = child;
                        jawBoneBaseX = child.rotation.x;
                    }
                }
            });
        }

        // Ultimate fallback: pick first mesh with any morphs and force it to work
        if(!jawMesh && jawOpenMorphTargetIndex===null && !jawBone){
            model.traverse(child=>{
                if(jawMesh) return;
                if(child.isMesh && child.morphTargetDictionary){
                    const keys = Object.keys(child.morphTargetDictionary);
                    if(keys.length > 0){ 
                        jawMesh=child; 
                        jawOpenMorphTargetIndex=child.morphTargetDictionary[keys[0]]; 
                        console.log('FORCING morph:', keys[0], 'on mesh:', child.name);
                    }
                }
            });
        }
        
        // Absolute last resort: pick any bone and animate it
        if(!jawMesh && jawOpenMorphTargetIndex===null && !jawBone){
            model.traverse(child=>{
                if(jawBone) return;
                if(child.isBone && child.name){
                    jawBone = child;
                    jawBoneBaseX = child.rotation.x;
                    console.log('FORCING bone:', child.name);
                    return;
                }
            });
        }

        // Report what control was detected for diagnostics
        if(jawMesh && jawOpenMorphTargetIndex!==null){
            console.log('[LipSync] Using morph target', bestMorphName || '(unknown)', 'on mesh', jawMesh.name, 'index', jawOpenMorphTargetIndex);
        } else if(jawBone){
            console.log('[LipSync] Using bone', jawBone.name);
        } else {
            console.warn('[LipSync] No jaw morph or bone detected. Consider setting LIP_MESH_NAME/LIP_MORPH_NAME or LIP_BONE_NAME.');
        }

        // no lip sync UI
        loadingOverlay.style.display='none';
    },xhr=>{},err=>{ loadingOverlay.style.display='none'; showStatusMessage('Failed to load 3D model'); });
}

function loadEngineModel(url){
    const loader = new GLTFLoader();
    loader.load(url, gltf => {
        engineModel = gltf.scene;
        scene.add(engineModel);
        engineModel.visible = true;
        // Scale engine much smaller relative to avatar
        engineModel.scale.set(0.3, 0.3, 0.3);
        // Initial placement to the right of the avatar in non-AR view
        if (model) {
            const offsetWorld = ENGINE_OFFSET.clone();
            offsetWorld.applyQuaternion(model.quaternion);
            engineModel.position.copy(model.position).add(offsetWorld);
            engineModel.quaternion.copy(model.quaternion);
        } else {
            engineModel.position.copy(ENGINE_OFFSET);
        }
    }, xhr => {}, err => {
        showStatusMessage('Failed to load engine model');
    });
}

// --- AR Placement --- //
function onSelect(){
    if(!reticle.visible) return;
    // Only allow first-time placement via tap. Later moves require long-press.
    if(!isPlaced && model){
        model.position.setFromMatrixPosition(reticle.matrix);
        model.visible=true;
        arInstructions.style.display='none';
        reticle.visible=false;
        isPlaced=true;
    }
}

// --- Reset Avatar --- //
function resetAvatar(){
    if(model){
        model.position.set(0,0,0);
        model.scale.set(1,1,1);
        model.rotation.set(0,0,0);
        jawOpenMorphTargetIndex!==null && setJaw(0);
        reticle.visible=true;
    }
    if(engineModel && model){
        const offsetWorld = ENGINE_OFFSET.clone();
        offsetWorld.applyQuaternion(model.quaternion);
        engineModel.position.copy(model.position).add(offsetWorld);
        engineModel.quaternion.copy(model.quaternion);
        // Keep engine at fixed small scale
        engineModel.scale.set(0.3, 0.3, 0.3);
    }
}

// --- Touch Gestures for Scale --- //
let ongoingTouches=[];
function getDistance(touches){ const dx=touches[0].clientX-touches[1].clientX; const dy=touches[0].clientY-touches[1].clientY; return Math.sqrt(dx*dx+dy*dy); }
function getAngle(touches){ const dx=touches[1].clientX - touches[0].clientX; const dy=touches[1].clientY - touches[0].clientY; return Math.atan2(dy, dx); }
function onTouchStart(e){
    // Two-finger pinch to scale
    if(e.touches.length===2 && model){
        initialScale.copy(model.scale); initialPinchDistance=getDistance(e.touches); initialTwoFingerAngle=getAngle(e.touches);
        if(longPressTimer){ clearTimeout(longPressTimer); longPressTimer=null; }
        allowReposition=false;
        return;
    }
    // One-finger long-press to enable reposition (only if already placed)
    if(e.touches.length===1 && renderer.xr.isPresenting && isPlaced){
        const t=e.touches[0];
        longPressStart={x:t.clientX,y:t.clientY};
        // Prepare for potential rotate if user drags
        isRotatingOneFinger=true; rotateStartX=t.clientX; rotateStartY=t.clientY; initialYaw = model ? model.rotation.y : 0;
        if(longPressTimer){ clearTimeout(longPressTimer); }
        longPressTimer=setTimeout(()=>{
            allowReposition=true; reticle.visible=true; arInstructions.style.display='flex';
        }, LONG_PRESS_MS);
    }
}
function onTouchMove(e){
    // Cancel long-press if finger moves too much
    if(e.touches.length===1 && longPressTimer){
        const t=e.touches[0];
        if(Math.abs(t.clientX-longPressStart.x)>MOVE_TOLERANCE || Math.abs(t.clientY-longPressStart.y)>MOVE_TOLERANCE){
            clearTimeout(longPressTimer); longPressTimer=null;
        }
    }
    if(e.touches.length===2 && model){
        const newDist=getDistance(e.touches); const scaleFactor=newDist/initialPinchDistance; model.scale.copy(initialScale.clone().multiplyScalar(scaleFactor));
        // Two-finger twist rotation
        const ang=getAngle(e.touches); const delta=ang-initialTwoFingerAngle; model.rotation.y = initialYaw + delta;
    }
    if(e.touches.length===1 && model && isRotatingOneFinger && !allowReposition){
        // Horizontal drag rotates around Y
        const t=e.touches[0];
        const dx=t.clientX - rotateStartX;
        const sensitivity=0.005; // radians per px
        model.rotation.y = initialYaw + dx * sensitivity;
    }
}
function onTouchEnd(e){
    if(longPressTimer){ clearTimeout(longPressTimer); longPressTimer=null; }
    if(e.touches.length<2) initialPinchDistance=0;
    // If long-press enabled reposition and reticle is visible, move model to reticle on release
    if(allowReposition && reticle.visible && model){
        model.position.setFromMatrixPosition(reticle.matrix);
        reticle.visible=false; arInstructions.style.display='none';
        allowReposition=false;
    }
    // End one-finger rotate state
    if(e.touches.length===0){ isRotatingOneFinger=false; }
}

// --- Window Resize --- //
function onWindowResize(){ camera.aspect=window.innerWidth/window.innerHeight; camera.updateProjectionMatrix(); renderer.setSize(window.innerWidth,window.innerHeight); }

// --- Animate Loop --- //
function animate(){
    const delta=clock.getDelta();
    if(mixer) mixer.update(delta);
    // Keep engine model locked to the avatar with a right-side offset
    if(engineModel && model){
        const offsetWorld = ENGINE_OFFSET.clone();
        offsetWorld.applyQuaternion(model.quaternion);
        engineModel.position.copy(model.position).add(offsetWorld);
        engineModel.quaternion.copy(model.quaternion);
        // Keep engine at fixed small scale
        engineModel.scale.set(0.3, 0.3, 0.3);
    }
    renderer.render(scene,camera);
    updateARHitTest();
    updateMicLevel();
}

function updateARHitTest(){
    if(renderer.xr.isPresenting && !hitTestSourceRequested){
        const session=renderer.xr.getSession();
        session.requestReferenceSpace('viewer').then(refSpace=>{
            session.requestHitTestSource({space:refSpace}).then(source=>{ hitTestSource=source; });
        });
        session.addEventListener('end',()=>{ hitTestSourceRequested=false; hitTestSource=null; });
        hitTestSourceRequested=true;
    }
    if(hitTestSource && renderer.xr.getSession()){
        const frame=renderer.xr.getFrame();
        const referenceSpace=renderer.xr.getReferenceSpace();
        const hitTestResults=frame.getHitTestResults(hitTestSource);
        if(hitTestResults.length>0){
            const pose=hitTestResults[0].getPose(referenceSpace);
            reticle.visible=true;
            reticle.position.set(pose.transform.position.x,pose.transform.position.y,pose.transform.position.z);
            reticle.updateMatrix();
        } else { reticle.visible=false; }
    }
}

// --- Mic Level --- //
function updateMicLevel(){
    if(analyser && isListening){
        analyser.getByteFrequencyData(dataArray);
        const avg=dataArray.reduce((a,b)=>a+b,0)/dataArray.length;
        micLevelFill.style.width=Math.min((avg/128*100),100)+'%';
    } else { micLevelFill.style.width='0%'; }
}
</script>
</body>
</html>
