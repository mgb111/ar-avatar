<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>AI Avatar Lip-Sync & AR</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            margin: 0;
            overflow: hidden;
            background-color: #111827;
            color: #f3f4f6;
        }
        canvas {
            display: block;
            width: 100%;
            height: 100%;
        }
        #ar-button-container {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        #ar-button {
            background-color: rgba(31, 41, 55, 0.8);
            color: white;
            padding: 1rem 2rem;
            font-size: 1.25rem;
            border-radius: 9999px;
            border: 1px solid rgba(75, 85, 99, 0.8);
            cursor: pointer;
            font-weight: 600;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            transition: all 0.2s ease-in-out;
            z-index: 20;
        }
        #ar-button:hover {
            background-color: rgba(55, 65, 81, 0.9);
        }
        #ar-button:active {
            transform: scale(0.95);
        }
        #loading-overlay, #ar-instructions {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            z-index: 100;
            color: #d1d5db;
            font-size: 1.125rem;
            background-color: rgba(17, 24, 39, 0.9);
            text-align: center;
            padding: 1rem;
        }
        #ar-instructions {
            display: none;
        }
        .loader, .thinking-loader {
            border: 4px solid #4b5563;
            border-top: 4px solid #60a5fa;
            border-radius: 50%;
            width: 40px;
            height: 40px;
            animation: spin 1s linear infinite;
        }
        .loader {
             margin-bottom: 1rem;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div id="loading-overlay">
        <div class="loader"></div>
        <span>Loading 3D environment...</span>
    </div>

    <div id="ar-instructions">
        <span>Move your phone to find a flat surface, then tap to place the avatar.</span>
    </div>

    <div id="ar-button-container">
        <button id="ar-button-placeholder">Enter Augmented Reality</button>
    </div>
    
    <div id="ar-controls-container" style="position: absolute; bottom: 2rem; left: 50%; transform: translateX(-50%); z-index: 10; display: none; flex-direction: column; align-items: center; gap: 1rem;">
        <div>
            <button id="mic-button" class="w-16 h-16 bg-blue-600 hover:bg-blue-700 text-white font-bold p-4 rounded-full transition duration-200 shadow-lg flex justify-center items-center">
                 <svg id="mic-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path><path d="M19 10v2a7 7 0 0 1-14 0v-2"></path><line x1="12" y1="19" x2="12" y2="23"></line></svg>
                 <div id="thinking-icon" class="thinking-loader" style="display:none; width: 24px; height: 24px;"></div>
            </button>
        </div>
        <input type="password" id="api-key-input" placeholder="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" class="bg-gray-800 border border-gray-600 text-white text-sm rounded-full px-4 py-2 w-64 text-center placeholder-gray-400" autocomplete="off">
    </div>

    <!-- ES Module Shims for older browser support -->
    <script async src="https://unpkg.com/es-module-shims@1.8.0/dist/es-module-shims.js"></script>
    <script type="importmap">
        {
            "imports": {
                "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
                "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/"
            }
        }
    </script>
    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
        import { ARButton } from 'three/addons/webxr/ARButton.js';

        let camera, scene, renderer, controls;
        let model, mixer, clock;
        let jawOpenMorphTargetIndex = null;
        let reticle;
        let voices = [];
        let speechRecognition;
        let isListening = false;
        let isSpeaking = false;
        let isAIThinking = false;
        
        const loadingOverlay = document.getElementById('loading-overlay');
        const arButtonContainer = document.getElementById('ar-button-container');
        const arInstructions = document.getElementById('ar-instructions');
        const arControlsContainer = document.getElementById('ar-controls-container');
        const micButton = document.getElementById('mic-button');
        const micIcon = document.getElementById('mic-icon');
        const thinkingIcon = document.getElementById('thinking-icon');
        const apiKeyInput = document.getElementById('api-key-input');

        const defaultAvatarUrl = 'https://raw.githubusercontent.com/mgb111/ar-avatar/main/avatar.glb';
        
        let activeGesture = null;
        let initialScale = new THREE.Vector3();
        let initialPinchDistance = 0;
        let groundPlane = null;
        const raycaster = new THREE.Raycaster();
        const touchPos = new THREE.Vector2();

        init();

        function init() {
            scene = new THREE.Scene();
            clock = new THREE.Clock();
            camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
            camera.position.set(0, 1.6, 2.5);

            const ambientLight = new THREE.AmbientLight(0xffffff, 1.5);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 2.5);
            directionalLight.position.set(1, 1, 1);
            scene.add(directionalLight);

            renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);
            renderer.xr.enabled = true;
            document.body.appendChild(renderer.domElement);

            controls = new OrbitControls(camera, renderer.domElement);
            controls.target.set(0, 1, 0);
            controls.update();
            controls.enabled = true;

            loadModel(defaultAvatarUrl);
            
            setupSpeechRecognition();

            micButton.addEventListener('click', toggleListening);
            window.addEventListener('resize', onWindowResize);
            renderer.domElement.addEventListener('touchstart', onTouchStart, { passive: false });
            renderer.domElement.addEventListener('touchmove', onTouchMove, { passive: false });
            renderer.domElement.addEventListener('touchend', onTouchEnd);
            
            setupAR();
            populateVoiceList();
            if (speechSynthesis.onvoiceschanged !== undefined) {
                speechSynthesis.onvoiceschanged = populateVoiceList;
            }
            renderer.setAnimationLoop(animate);
        }

        function populateVoiceList() {
            voices = window.speechSynthesis.getVoices();
        }

        function setupAR() {
            const arButton = ARButton.createButton(renderer, { 
                requiredFeatures: ['hit-test', 'dom-overlay'],
                domOverlay: { root: document.body }
            });
            arButton.id = 'ar-button';
            arButton.textContent = "Enter Augmented Reality";
            
            arButton.addEventListener('click', () => {
                window.speechSynthesis.cancel();
            });

            renderer.xr.addEventListener('sessionstart', () => {
                arButtonContainer.style.display = 'none';
                arInstructions.style.display = 'flex';
                controls.enabled = false;
            });
            
            renderer.xr.addEventListener('sessionend', () => {
                 arButtonContainer.style.display = 'flex';
                 arControlsContainer.style.display = 'none';
                 arInstructions.style.display = 'none';
                 window.speechSynthesis.cancel();
                 controls.enabled = true;
                 if(model) {
                   model.position.set(0, 0, 0);
                   model.visible = true;
                }
            });

            arButtonContainer.innerHTML = '';
            arButtonContainer.appendChild(arButton);

            reticle = new THREE.Mesh(
                new THREE.RingGeometry(0.05, 0.07, 32).rotateX(-Math.PI / 2),
                new THREE.MeshBasicMaterial()
            );
            reticle.matrixAutoUpdate = false;
            reticle.visible = false;
            scene.add(reticle);

            let controller = renderer.xr.getController(0);
            controller.addEventListener('select', onSelect);
            scene.add(controller);
        }
        
        function setupSpeechRecognition() {
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (SpeechRecognition) {
                speechRecognition = new SpeechRecognition();
                speechRecognition.continuous = false;
                speechRecognition.lang = 'en-US';
                speechRecognition.interimResults = false;
                speechRecognition.maxAlternatives = 1;

                speechRecognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    getAIResponse(transcript);
                };

                speechRecognition.onstart = () => {
                    isListening = true;
                    micButton.classList.add('bg-red-600', 'hover:bg-red-700');
                    micButton.classList.remove('bg-blue-600', 'hover:bg-blue-700');
                };
                
                speechRecognition.onend = () => {
                    isListening = false;
                    micButton.classList.remove('bg-red-600', 'hover:bg-red-700');
                    micButton.classList.add('bg-blue-600', 'hover:bg-blue-700');
                };

                speechRecognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                };
            } else {
                console.warn('Speech Recognition not supported in this browser.');
                micButton.disabled = true;
            }
        }
        
        function toggleListening() {
            if (isListening || isSpeaking || isAIThinking) return;
            window.speechSynthesis.cancel();
            speechRecognition.start();
        }
        
        async function getAIResponse(userPrompt) {
            const finalApiKey = apiKeyInput.value;

            if (!finalApiKey) {
                speak("I need an API key to connect. Please enter your OpenAI API key to chat.");
                return;
            }

            isAIThinking = true;
            micIcon.style.display = 'none';
            thinkingIcon.style.display = 'block';
            micButton.disabled = true;

            const systemPrompt = `You are a helpful AI assistant for QuickScanAR, a tool that converts static marketing print materials into interactive augmented reality experiences. Your name is Manish. Your goal is to be helpful and informative about QuickScanAR. Your core information is: "Hello, I am Manish from QuickScanAR, a tool that converts your static marketing print materials into interactive augmented reality experiences. I look forward to serving your immersive media needs.". Respond to user questions based on this context. Be concise and conversational.`;
            
            const apiUrl = 'https://api.openai.com/v1/chat/completions';
            const payload = {
                model: "gpt-3.5-turbo",
                messages: [
                    { "role": "system", "content": systemPrompt },
                    { "role": "user", "content": userPrompt }
                ]
            };

            try {
                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Authorization': `Bearer ${finalApiKey}`
                    },
                    body: JSON.stringify(payload)
                });
                if (!response.ok) {
                    const errorBody = await response.json();
                    console.error("API Error Body:", errorBody);
                    throw new Error(`API call failed with status: ${response.status}. Please check your API key.`);
                }
                const result = await response.json();
                const text = result.choices?.[0]?.message?.content;
                if (text) {
                    speak(text.trim());
                } else {
                   speak("I'm sorry, I couldn't come up with a response.");
                }
            } catch (error) {
                console.error("Error calling OpenAI API:", error);
                speak("I seem to be having a problem connecting. Please check your OpenAI API key and try again later.");
            } finally {
                isAIThinking = false;
                micIcon.style.display = 'block';
                thinkingIcon.style.display = 'none';
                micButton.disabled = false;
            }
        }

        function onSelect() {
            if (reticle.visible && model) {
                model.position.setFromMatrixPosition(reticle.matrix);
                model.visible = true;
                groundPlane = new THREE.Plane(new THREE.Vector3(0, 1, 0), -model.position.y);
                arInstructions.style.display = 'none';
                arControlsContainer.style.display = 'flex';
            }
        }
        
        let hitTestSource = null;
        let hitTestSourceRequested = false;

        function animate(timestamp, frame) {
            if (!renderer.xr.isPresenting) {
                controls.update();
            }
            if (mixer) mixer.update(clock.getDelta());
            
            handleLipSync();
            
            if (frame) {
                const referenceSpace = renderer.xr.getReferenceSpace();
                const session = renderer.xr.getSession();

                if (hitTestSourceRequested === false) {
                    session.requestReferenceSpace('viewer').then(referenceSpace => {
                        session.requestHitTestSource({ space: referenceSpace }).then(source => hitTestSource = source);
                    });
                    session.addEventListener('end', () => {
                        hitTestSourceRequested = false;
                        hitTestSource = null;
                    });
                    hitTestSourceRequested = true;
                }

                if (hitTestSource) {
                    const hitTestResults = frame.getHitTestResults(hitTestSource);
                    if (hitTestResults.length) {
                        const hit = hitTestResults[0];
                        reticle.visible = true;
                        reticle.matrix.fromArray(hit.getPose(referenceSpace).transform.matrix);
                    } else {
                        reticle.visible = false;
                    }
                }
            }
            renderer.render(scene, camera);
        }

        function loadModel(url, onLoadedCallback = () => {}) {
            loadingOverlay.style.display = 'flex';
            const loader = new GLTFLoader();
            if(model) scene.remove(model);
            loader.load(url, (gltf) => {
                model = gltf.scene;
                const box = new THREE.Box3().setFromObject(model);
                const center = box.getCenter(new THREE.Vector3());
                const size = box.getSize(new THREE.Vector3());
                const maxDim = Math.max(size.x, size.y, size.z);
                const scale = 2.0 / maxDim;
                model.scale.set(scale, scale, scale);
                model.position.sub(center.multiplyScalar(scale));
                scene.add(model);
                model.traverse((node) => {
                    if (node.isMesh && node.morphTargetDictionary) {
                        const names = ['jawOpen', 'mouthOpen'];
                        for (const name of names) {
                            if (node.morphTargetDictionary[name] !== undefined) {
                                jawOpenMorphTargetIndex = node.morphTargetDictionary[name];
                                break;
                            }
                        }
                    }
                });
                if (gltf.animations && gltf.animations.length) {
                    mixer = new THREE.AnimationMixer(model);
                    mixer.clipAction(gltf.animations[0]).play();
                }
                loadingOverlay.style.display = 'none';
                model.visible = !renderer.xr.isPresenting;
                onLoadedCallback();
            }, undefined, (error) => console.error(error));
        }
        
        function speak(text) {
            if (!text || !model) return;
            if (jawOpenMorphTargetIndex === null) console.warn("Lip-sync disabled.");
            
            window.speechSynthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);

            if (voices.length === 0) {
                voices = window.speechSynthesis.getVoices();
            }

            const maleVoiceKeywords = ['Male', 'David', 'Daniel', 'Fred', 'Tom', 'Google US English'];
            let selectedVoice = null;

            for (const keyword of maleVoiceKeywords) {
                selectedVoice = voices.find(v => v.lang.startsWith('en') && v.name.includes(keyword));
                if (selectedVoice) break;
            }

            if (selectedVoice) {
                utterance.voice = selectedVoice;
            } else {
                const fallbackVoice = voices.find(v => v.lang === 'en-US');
                if(fallbackVoice) {
                    utterance.voice = fallbackVoice;
                }
                console.warn("A specific male voice was not found. Using a default English voice.");
            }
            
            utterance.onstart = () => { isSpeaking = true; };
            utterance.onend = () => { isSpeaking = false; };
            window.speechSynthesis.speak(utterance);
        }

        function handleLipSync() {
            if (isSpeaking && jawOpenMorphTargetIndex !== null) {
                const influence = (Math.sin(Date.now() * 0.025) + 1) / 2;
                model.traverse(c => c.isMesh && c.morphTargetInfluences && (c.morphTargetInfluences[jawOpenMorphTargetIndex] = influence * 0.8));
            } else if (model && jawOpenMorphTargetIndex !== null) {
                model.traverse(c => c.isMesh && c.morphTargetInfluences && (c.morphTargetInfluences[jawOpenMorphTargetIndex] = 0));
            }
        }

        function onTouchStart(event) {
            if (!renderer.xr.isPresenting || !model || !model.visible) return;
            if (event.touches.length === 1) activeGesture = 'pan';
            else if (event.touches.length === 2) {
                activeGesture = 'scale';
                initialScale.copy(model.scale);
                initialPinchDistance = getPinchDistance(event.touches);
            }
        }

        function onTouchMove(event) {
            if (!renderer.xr.isPresenting || !model || !model.visible || !activeGesture) return;
            event.preventDefault();
            if (activeGesture === 'pan' && event.touches.length === 1) {
                touchPos.x = (event.touches[0].clientX / window.innerWidth) * 2 - 1;
                touchPos.y = -(event.touches[0].clientY / window.innerHeight) * 2 + 1;
                raycaster.setFromCamera(touchPos, camera);
                const intersectPoint = new THREE.Vector3();
                raycaster.ray.intersectPlane(groundPlane, intersectPoint);
                if (intersectPoint) model.position.set(intersectPoint.x, model.position.y, intersectPoint.z);
            } else if (activeGesture === 'scale' && event.touches.length === 2) {
                const dist = getPinchDistance(event.touches);
                const scaleFactor = dist / initialPinchDistance;
                model.scale.copy(initialScale).multiplyScalar(scaleFactor);
            }
        }

        function onTouchEnd(event) {
            if (event.touches.length < 2) activeGesture = null;
            if (event.touches.length === 1) activeGesture = 'pan';
        }

        function getPinchDistance(touches) {
            const dx = touches[0].clientX - touches[1].clientX;
            const dy = touches[0].clientY - touches[1].clientY;
            return Math.sqrt(dx * dx + dy * dy);
        }

        function onWindowResize() {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        }
    </script>
</body>
</html>

